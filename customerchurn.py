# -*- coding: utf-8 -*-
"""CustomerChurn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dF29VP927xCqpxUHVHb_9aMCta20D98A
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install keras-tuner

!pip install scikeras

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.layers import Input, Dense
from keras import models
from tensorflow.keras import layers
import kerastuner as kt
from kerastuner.tuners import RandomSearch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
from keras.models import Model
from keras.layers import Input, Dense
from sklearn.model_selection import GridSearchCV
from sklearn import datasets
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold
from keras import regularizers
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""**STEP 1: READ THE DATASET AND CLEAN IT**"""

#Read the file
df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/CustomerChurn_dataset.csv')
df

#Drop useless columns such as the customerID.
df.drop("customerID", axis=1, inplace=True)
df.info()

#Convert non-numeric values such as Totalcharge to numeric.
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df.info()

#Identify columns with categorical values.
Categorical_values = df.select_dtypes(include=['object', 'category']).columns
Categorical_values

#Encode the categorical columns to 0's and 1's

for column in Categorical_values:
 df[column], _ = pd.factorize(df[column])
encoded_columns = df[Categorical_values]
encoded_columns

"""**STEP 2: EXTRACT FILE FEATURES THAT CAN DEFINE A CUSTOMER CHURN:****"""

# Calculate the correlation matrix (CustomerChurn dataset)
correlation_matrix_customer = encoded_columns.corr()

# Number of top features to select
N = 17

# Getting the top N features with the highest absolute correlation with 'Churn'
top_correlated_features = correlation_matrix_customer['Churn'].abs().nlargest(N + 1).index

# Remove 'Churn' from the list of top correlated features
top_correlated_features = top_correlated_features.drop('Churn')

# Creating a feature subset with the selected top features (the independent variables)
feature_subset = encoded_columns[top_correlated_features]


# Define the dependent variable which is 'Churn'
dependent_variable = encoded_columns['Churn']

#Display the top correlated features.
top_correlated_features

# Concatenate the encoded categorical variables with numeric data to the full list of independent variables.
numerical_columns = ['MonthlyCharges', 'TotalCharges']
feature_subset_conc_data = pd.concat([encoded_columns[top_correlated_features], df[numerical_columns]], axis=1)
feature_subset_conc_data

#Visualization of all categorical and independent variables against Churn Rate.

#categorical variables:
feature_subset_conc_data = ['gender', 'SeniorCitizen', 'Partner', 'Dependents','PhoneService', 'MultipleLines', 'InternetService',
                      'OnlineBackup', 'DeviceProtection', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',
                      'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']

analysis_data = df[feature_subset_conc_data]


plt.figure(figsize=(18, 15))
for i, column in enumerate(analysis_data.columns[:-1]):
    plt.subplot(8, 2, i + 1)
    sns.countplot(x=column, hue='Churn', data=analysis_data)
    plt.title(f'Distribution of {column}')
    plt.xlabel('')
    plt.ylabel('Count')

plt.tight_layout()
plt.show()

#Data Visualization for the categorical variables
feature_subset_conc_data.hist(bins=50, figsize=(20,15))
plt.show()

from sklearn.preprocessing import StandardScaler, MinMaxScaler


#Scaling independent variables
columns_to_scale = top_correlated_features
scaled_data = feature_subset_conc_data[columns_to_scale]

# Create an instance of the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the data and transform the selected columns
scaled_data = scaler.fit_transform(scaled_data)


  # Replace the original columns with the scaled data in the DataFrame
feature_subset_conc_data[columns_to_scale] = scaled_data
scaled_data

"""**STEP 3: USE YOUR EDA(EXPLORATORY DATA ANALYSIS) SKILLS TO FIND OUT WHICH CUSTOMER PROFILES RELATE TO CHURNING A LOT.**"""

#feature_subset represents the independent variables.
#Obtaining x and y values
y_values = encoded_columns['Churn']
x_values = feature_subset[top_correlated_features]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.2, random_state=42)
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""**STEP 4: USING THE FEATURES IN (1) DEFINE AND TRAIN A MULTI-LAYER PERCEPTRON MODEL USING THE FUNCTIONAL API**

"""

def create_keras_model(optimizer = 'adam', hidden_layer_1_unit = 32, hidden_layer_2_unit = 16):
    input_layer = Input(shape=15,)
    # Define hidden layers with the specified number of units (neurons) and activation functions
    hidden_layer_1 = Dense(hidden_layer_1_unit, activation='relu')(input_layer)
    hidden_layer_2 = Dense(hidden_layer_2_unit, activation='relu')(hidden_layer_1)

    output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)

    model = Model(inputs=input_layer, outputs=output_layer)

    # Compile the model
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Create a KerasClassifier based on your function
keras_model = KerasClassifier(model=create_keras_model,epochs=10, batch_size=32, verbose=0, hidden_layer_1_unit = 32, hidden_layer_2_unit = 16)

# Define hyperparameters for grid search
param_grid = {
    'optimizer': ['adam', 'sgd', 'rmsprop'],
    'hidden_layer_1_unit': [32, 64, 128],
    'hidden_layer_2_unit': [16, 32, 64],
}

"""**USING GRIDSEARCH TO FIND THE OPTIMAL PARAMETERS**"""

# Use GridSearchCV to find the optimal hyperparameters
grid = GridSearchCV(estimator=keras_model, param_grid=param_grid, scoring='roc_auc', cv=3, error_score='raise')
grid_result = grid.fit(X_train, y_train)

# Get the best parameters
best_params = grid_result.best_params_

# Display the results
print(f"Best Parameters: {best_params}")

"""**STEP 5: EVALUATE THE MODELâ€™S ACCURACY AND CALCULATE THE AUC SCORE**"""

from sklearn.metrics import accuracy_score, roc_auc_score

# Get the best parameters
best_params = grid_result.best_params_

# Create a new KerasClassifier with the best parameters
best_keras_model = KerasClassifier(model=create_keras_model, **best_params, epochs=10, batch_size=32, verbose=0)

# Train the model with the best parameters
best_keras_model.fit(X_train, y_train)

# Predict on the test set
y_pred = best_keras_model.predict(X_test)

# Evaluate accuracy and AUC score
accuracy = accuracy_score(y_test, y_pred)
auc_score = roc_auc_score(y_test, y_pred)

# Display the results
print(f"Accuracy: {accuracy}")
print(f"AUC Score: {auc_score}")

"""**OPTIMIZING WITH THE BEST PARAMETERS TO GET THE FINAL ACCURACY AND AUC SCORES**"""

from sklearn.metrics import accuracy_score, roc_auc_score
from tensorflow.keras.optimizers import SGD

input_layer = Input(shape=19,)
# Define hidden layers with the specified number of units (neurons) and activation functions
hidden_layer_1 = Dense(64, activation='relu')(input_layer)
hidden_layer_2 = Dense(32, activation='relu')(hidden_layer_1)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)

Opt_model = Model(inputs=input_layer, outputs=output_layer)

    # Compile the model
Opt_model.compile(optimizer= SGD(), loss='binary_crossentropy', metrics=['accuracy'])
# Train the model with the best parameters
Opt_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

# Predict on the test set
y_pred_optimized = best_keras_model.predict(X_test)
y_pred_opt_binary=(y_pred > 0.5).astype(int)

# Evaluate accuracy and AUC score
accuracy = accuracy_score(y_test,  y_pred_opt_binary)
auc_score = roc_auc_score(y_test, y_pred_optimized)

# Display the results
print(f"Accuracy: {accuracy}")
print(f"AUC Score: {auc_score}")

"""**SAVING THE MODEL**"""

import pickle

# Save the model
with open('Customer_Churn_Model.pkl', 'wb') as file_name:
    pickle.dump(best_keras_model, file_name)

import pickle
with open('scaler.pkl', 'wb') as scaler_file:
  pickle.dump(scaler, scaler_file)