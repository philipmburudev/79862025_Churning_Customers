# -*- coding: utf-8 -*-
"""CustomerChurn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dF29VP927xCqpxUHVHb_9aMCta20D98A
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from keras.models import Model
from keras.layers import Input, Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
from sklearn import datasets
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold
from keras import regularizers
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""**STEP 1: READ THE DATASET AND CLEAN IT**"""

#Read the file
df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/CustomerChurn_dataset.csv')
df

#Drop useless columns such as the customerID.
df.drop("customerID", axis=1, inplace=True)
df.info()

#Convert non-numeric values such as Totalcharge to numeric.
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df.info()

#Identify columns with categorical values.
Categorical_values = df.select_dtypes(include=['object', 'category']).columns
Categorical_values

#Encode the categorical columns to 0's and 1's

for column in Categorical_values:
 df[column], _ = pd.factorize(df[column])
encoded_columns = df[Categorical_values]
encoded_columns

"""**STEP 2: EXTRACT FILE FEATURES THAT CAN DEFINE A CUSTOMER CHURN:****"""

# Calculate the correlation matrix (CustomerChurn dataset)
correlation_matrix_customer = encoded_columns.corr()

# Number of top features to select
N = 17

# Getting the top N features with the highest absolute correlation with 'Churn'
top_correlated_features = correlation_matrix_customer['Churn'].abs().nlargest(N + 1).index

# Remove 'Churn' from the list of top correlated features
top_correlated_features = top_correlated_features.drop('Churn')

# Creating a feature subset with the selected top features (the independent variables)
feature_subset = encoded_columns[top_correlated_features]


# Define the dependent variable which is 'Churn'
dependent_variable = encoded_columns['Churn']

#Display the top correlated features.
top_correlated_features

# Concatenate the encoded categorical variables with numeric data to the full list of independent variables.
numerical_columns = ['MonthlyCharges', 'TotalCharges']
feature_subset_conc_data = pd.concat([encoded_columns[top_correlated_features], df[numerical_columns]], axis=1)
feature_subset_conc_data

#Visualization of all categorical and independent variables against Churn Rate.

#categorical variables:
feature_subset_conc_data = ['gender', 'SeniorCitizen', 'Partner', 'Dependents','PhoneService', 'MultipleLines', 'InternetService',
                      'OnlineBackup', 'DeviceProtection', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',
                      'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']

analysis_data = df[feature_subset_conc_data]


plt.figure(figsize=(18, 15))
for i, column in enumerate(analysis_data.columns[:-1]):
    plt.subplot(8, 2, i + 1)
    sns.countplot(x=column, hue='Churn', data=analysis_data)
    plt.title(f'Distribution of {column}')
    plt.xlabel('')
    plt.ylabel('Count')

plt.tight_layout()
plt.show()

#Data Visualization for the categorical variables
feature_subset_conc_data.hist(bins=50, figsize=(20,15))
plt.show()

from sklearn.preprocessing import StandardScaler, MinMaxScaler


#Scaling independent variables
columns_to_scale = top_correlated_features
scaled_data = feature_subset_conc_data[columns_to_scale]

# Create an instance of the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the data and transform the selected columns
scaled_data = scaler.fit_transform(scaled_data)


  # Replace the original columns with the scaled data in the DataFrame
feature_subset_conc_data[columns_to_scale] = scaled_data
scaled_data

"""**STEP 3: USE YOUR EDA(EXPLORATORY DATA ANALYSIS) SKILLS TO FIND OUT WHICH CUSTOMER PROFILES RELATE TO CHURNING A LOT.**"""

#feature_subset represents the independent variables.
#Obtaining x and y values
y_values = encoded_columns['Churn']
x_values = feature_subset[top_correlated_features]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.2, random_state=42)
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""**STEP 4: USING THE FEATURES IN (1) DEFINE AND TRAIN A MULTI-LAYER PERCEPTRON MODEL USING THE FUNCTIONAL API**

"""

# Define a function to create the Keras model using the Functional API
def create_model(hidden_layer_sizes=(10,), activation='relu', alpha=0.0001):
    inputs = Input(shape=(X_train.shape[1],))
    x = inputs
    for layer_size in hidden_layer_sizes:
        x = Dense(layer_size, activation=activation, kernel_regularizer=regularizers.l2(alpha))(x)
    outputs = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Function to perform GridSearchCV with Keras model
def grid_search_keras_model(X_train, y_train):
    # Wrap the Keras model into a scikit-learn compatible classifier
    keras_classifier = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)

    # Define the parameter grid for GridSearchCV
    param_grid = {
        'hidden_layer_sizes': [(10,), (50,), (100,)],
        'activation': ['relu', 'tanh', 'sigmoid'],
        'alpha': [0.0001, 0.001, 0.01],
    }

    # Create GridSearchCV object
    grid_search = GridSearchCV(keras_classifier, param_grid, cv=StratifiedKFold(n_splits=5), n_jobs=-1)

    # Perform GridSearchCV with cross-validation
    grid_search.fit(X_train, y_train)

    # Print the best parameters and corresponding accuracy
    print("Best Parameters: ", grid_search.best_params_)
    print("Best Cross-Validation Accuracy: {:.2f}%".format(grid_search.best_score_ * 100))

    return grid_search

# Load or create your dataset
# For example, let's use the breast cancer dataset from sklearn
data = datasets.load_breast_cancer()
X = data.data
y = data.target

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Perform GridSearchCV
grid_search = grid_search_keras_model(X_train, y_train)

# Evaluate the best model on the test set
test_predictions = grid_search.best_estimator_.predict(X_test)
test_accuracy = accuracy_score(y_test, test_predictions)
print("Test Set Accuracy: {:.2f}%".format(test_accuracy * 100))

"""**STEP 5: EVALUATE THE MODELâ€™S ACCURACY AND CALCULATE THE AUC SCORE**"""

from sklearn.metrics import accuracy_score, roc_auc_score

# ... (previous code remains the same)

# Perform GridSearchCV
grid_search = grid_search_keras_model(X_train, y_train)

# Evaluate the best model on the test set
test_predictions_proba = grid_search.best_estimator_.predict_proba(X_test)  # Predict probabilities
test_predictions = (test_predictions_proba[:, 1] > 0.5).astype(int)  # Convert probabilities to binary predictions

# Calculate accuracy
test_accuracy = accuracy_score(y_test, test_predictions)
print("Test Set Accuracy: {:.2f}%".format(test_accuracy * 100))

# Calculate AUC score
test_auc = roc_auc_score(y_test, test_predictions_proba[:, 1])
print("Test Set AUC Score: {:.2f}".format(test_auc))



import pickle

# Save the model
with open('Customer_Churn_Rate.pkl', 'wb') as file_name:
    pickle.dump(mlp, file_name)

X_train.shape()